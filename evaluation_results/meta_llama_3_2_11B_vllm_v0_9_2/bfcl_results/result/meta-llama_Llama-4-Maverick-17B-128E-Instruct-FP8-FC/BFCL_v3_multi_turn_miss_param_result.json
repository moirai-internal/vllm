{"id": "multi_turn_miss_param_0", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_1", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_2", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_3", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_4", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_5", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_6", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_7", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_8", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_9", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_10", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_11", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_12", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_13", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_14", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_15", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_16", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_17", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_18", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_19", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_20", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_21", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_22", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_23", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_24", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_25", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_26", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_27", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_28", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_29", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_30", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_31", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_32", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_33", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_34", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_35", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_36", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_37", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_38", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_39", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_40", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_41", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_42", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_43", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_44", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_45", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_46", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_47", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_48", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_49", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_50", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_51", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_52", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_53", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_54", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_55", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_56", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_57", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_58", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_59", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_60", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_61", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_62", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_63", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_64", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_65", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_66", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_67", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_68", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_69", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_70", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_71", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_72", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_73", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_74", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_75", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_76", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_77", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_78", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_79", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_80", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_81", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_82", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_83", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_84", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_85", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_86", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_87", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_88", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_89", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_90", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_91", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_92", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_93", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_94", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_95", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_96", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_97", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_98", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_99", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_100", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_101", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_102", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_103", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_104", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_105", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_106", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_107", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_108", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_109", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_110", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_111", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_112", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_113", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_114", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_115", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_116", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_117", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_118", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_119", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_120", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_121", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_122", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_123", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_124", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_125", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_126", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_127", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_128", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_129", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_130", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_131", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_132", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_133", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_134", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_135", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_136", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_137", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_138", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_139", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_140", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_141", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_142", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_143", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_144", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_145", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_146", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_147", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_148", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_149", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_150", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_151", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_152", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_153", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_154", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_155", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_156", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_157", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_158", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_159", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_160", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_161", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_162", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_163", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_164", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_165", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_166", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_167", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_168", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_169", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_170", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_171", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_172", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_173", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_174", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_175", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_176", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_177", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_178", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_179", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_180", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_181", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_182", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_183", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_184", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_185", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_186", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_187", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_188", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_189", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_190", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_191", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_192", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_193", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_194", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_195", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_196", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_197", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_198", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_miss_param_199", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
