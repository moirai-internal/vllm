{"id": "parallel_0", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_1", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_2", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_3", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_4", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_5", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_6", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_7", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_8", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_9", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_10", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_11", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_12", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_13", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_14", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_15", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_16", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_17", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_18", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_19", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_20", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_21", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_22", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_23", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_24", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_25", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_26", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_27", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_28", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_29", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_30", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_31", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_32", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_33", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_34", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_35", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_36", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_37", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_38", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_39", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_40", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_41", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_42", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_43", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_44", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_45", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_46", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_47", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_48", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_49", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_50", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_51", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_52", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_53", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_54", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_55", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_56", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_57", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_58", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_59", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_60", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_61", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_62", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_63", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_64", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_65", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_66", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_67", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_68", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_69", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_70", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_71", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_72", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_73", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_74", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_75", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_76", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_77", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_78", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_79", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_80", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_81", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_82", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_83", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_84", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_85", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_86", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_87", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_88", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_89", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_90", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_91", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_92", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_93", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_94", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_95", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_96", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_97", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_98", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_99", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_100", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_101", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_102", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_103", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_104", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_105", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_106", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_107", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_108", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_109", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_110", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_111", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_112", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_113", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_114", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_115", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_116", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_117", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_118", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_119", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_120", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_121", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_122", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_123", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_124", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_125", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_126", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_127", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_128", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_129", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_130", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_131", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_132", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_133", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_134", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_135", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_136", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_137", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_138", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_139", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_140", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_141", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_142", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_143", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_144", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_145", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_146", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_147", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_148", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_149", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_150", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_151", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_152", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_153", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_154", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_155", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_156", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_157", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_158", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_159", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_160", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_161", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_162", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_163", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_164", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_165", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_166", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_167", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_168", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_169", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_170", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_171", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_172", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_173", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_174", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_175", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_176", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_177", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_178", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_179", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_180", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_181", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_182", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_183", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_184", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_185", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_186", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_187", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_188", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_189", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_190", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_191", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_192", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_193", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_194", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_195", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_196", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_197", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_198", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "parallel_199", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
