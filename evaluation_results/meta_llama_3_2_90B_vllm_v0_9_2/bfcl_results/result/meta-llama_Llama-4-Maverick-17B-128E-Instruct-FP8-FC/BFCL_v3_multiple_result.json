{"id": "multiple_0", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_1", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_2", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_3", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_4", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_5", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_6", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_7", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_8", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_9", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_10", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_11", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_12", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_13", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_14", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_15", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_16", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_17", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_18", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_19", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_20", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_21", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_22", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_23", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_24", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_25", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_26", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_27", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_28", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_29", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_30", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_31", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_32", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_33", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_34", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_35", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_36", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_37", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_38", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_39", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_40", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_41", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_42", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_43", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_44", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_45", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_46", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_47", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_48", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_49", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_50", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_51", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_52", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_53", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_54", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_55", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_56", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_57", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_58", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_59", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_60", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_61", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_62", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_63", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_64", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_65", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_66", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_67", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_68", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_69", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_70", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_71", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_72", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_73", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_74", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_75", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_76", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_77", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_78", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_79", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_80", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_81", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_82", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_83", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_84", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_85", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_86", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_87", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_88", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_89", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_90", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_91", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_92", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_93", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_94", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_95", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_96", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_97", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_98", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_99", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_100", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_101", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_102", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_103", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_104", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_105", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_106", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_107", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_108", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_109", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_110", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_111", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_112", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_113", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_114", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_115", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_116", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_117", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_118", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_119", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_120", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_121", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_122", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_123", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_124", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_125", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_126", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_127", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_128", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_129", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_130", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_131", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_132", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_133", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_134", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_135", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_136", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_137", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_138", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_139", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_140", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_141", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_142", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_143", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_144", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_145", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_146", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_147", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_148", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_149", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_150", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_151", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_152", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_153", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_154", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_155", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_156", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_157", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_158", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_159", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_160", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_161", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_162", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_163", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_164", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_165", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_166", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_167", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_168", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_169", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_170", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_171", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_172", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_173", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_174", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_175", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_176", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_177", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_178", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_179", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_180", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_181", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_182", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_183", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_184", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_185", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_186", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_187", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_188", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_189", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_190", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_191", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_192", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_193", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_194", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_195", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_196", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_197", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_198", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multiple_199", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
