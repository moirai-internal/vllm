{"id": "javascript_0", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_1", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_2", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_3", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_4", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_5", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_6", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_7", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_8", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_9", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_10", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_11", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_12", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_13", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_14", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_15", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_16", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_17", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_18", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_19", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_20", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_21", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_22", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_23", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_24", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_25", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_26", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_27", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_28", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_29", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_30", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_31", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_32", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_33", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_34", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_35", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_36", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_37", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_38", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_39", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_40", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_41", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_42", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_43", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_44", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_45", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_46", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_47", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_48", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "javascript_49", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 628, in inference_single_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
