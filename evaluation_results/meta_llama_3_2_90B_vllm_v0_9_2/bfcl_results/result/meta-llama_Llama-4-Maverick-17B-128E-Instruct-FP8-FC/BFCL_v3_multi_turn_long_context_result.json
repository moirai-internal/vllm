{"id": "multi_turn_long_context_0", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_1", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_2", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_3", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_4", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_5", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_6", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_7", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_8", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_9", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_10", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_11", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_12", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_13", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_14", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_15", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_16", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_17", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_18", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_19", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_20", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_21", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_22", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_23", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_24", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_25", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_26", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_27", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_28", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_29", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_30", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_31", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_32", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_33", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_34", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_35", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_36", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_37", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_38", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_39", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_40", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_41", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_42", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_43", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_44", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_45", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_46", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_47", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_48", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_49", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_50", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_51", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_52", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_53", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_54", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_55", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_56", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_57", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_58", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_59", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_60", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_61", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_62", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_63", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_64", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_65", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_66", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_67", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_68", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_69", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_70", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_71", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_72", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_73", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_74", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_75", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_76", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_77", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_78", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_79", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_80", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_81", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_82", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_83", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_84", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_85", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_86", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_87", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_88", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_89", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_90", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_91", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_92", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_93", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_94", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_95", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_96", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_97", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_98", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_99", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_100", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_101", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_102", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_103", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_104", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_105", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_106", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_107", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_108", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_109", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_110", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_111", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_112", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_113", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_114", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_115", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_116", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_117", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_118", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_119", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_120", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_121", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_122", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_123", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_124", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_125", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_126", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_127", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_128", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_129", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_130", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_131", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_132", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_133", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_134", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_135", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_136", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_137", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_138", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_139", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_140", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_141", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_142", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_143", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_144", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_145", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_146", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_147", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_148", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_149", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_150", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_151", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_152", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_153", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_154", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_155", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_156", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_157", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_158", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_159", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_160", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_161", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_162", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_163", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_164", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_165", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_166", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_167", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_168", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_169", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_170", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_171", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_172", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_173", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_174", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_175", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_176", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_177", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_178", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_179", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_180", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_181", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_182", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_183", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_184", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_185", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_186", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_187", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_188", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_189", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_190", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_191", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_192", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_193", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_194", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_195", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_196", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_197", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_198", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
{"id": "multi_turn_long_context_199", "result": "Error during inference: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}", "traceback": "Traceback (most recent call last):\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 275, in _multi_threaded_inference\n    model_responses, metadata = self.inference_multi_turn_prompting(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/base_handler.py\", line 425, in inference_multi_turn_prompting\n    api_response, query_latency = self._query_prompting(inference_data)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 354, in _query_prompting\n    api_response = self.client.completions.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/resources/completions.py\", line 541, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/vasheno/miniconda3/envs/sanity_check/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'object': 'error', 'message': 'The model `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}\n"}
